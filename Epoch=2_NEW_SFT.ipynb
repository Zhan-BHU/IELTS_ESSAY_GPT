{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7236a20e-8837-4055-8edd-e2699cb0c26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  9 01:32:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:31:00.0 Off |                  Off |\n",
      "| 30%   33C    P5             62W /  450W |       1MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361ebf77-ee6b-4396-8c38-ff319f57647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b030e9f-e9d1-49d4-b50f-5735b691f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928098ef-6e4e-4d74-8400-54e913027a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/qwen/Qwen3-4B-Instruct-2507\n",
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/qwen/Qwen3-4B-Instruct-2507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c744faf67d4d5bb346074c16a2f03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "model_id = \"qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=\"/root/autodl-tmp\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=\"/root/autodl-tmp\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e63c5f-34ca-4447-9c81-602396b6f463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,515,072 || all params: 4,038,983,168 || trainable%: 0.4089\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9aaa79a-4262-477e-a29b-6ae296b0993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    task = example[\"task\"]\n",
    "    ctx = example[\"context\"]\n",
    "\n",
    "    # 根据任务类型构建上下文\n",
    "    if task == \"IELTS_TASK1_POLISH\":\n",
    "        # Task1 有 topic、subject（可能为空）、image_description（可能为空）\n",
    "        parts = []\n",
    "        if ctx.get(\"topic\"):\n",
    "            parts.append(f\"Topic: {ctx['topic']}\")\n",
    "        if ctx.get(\"subject\"):\n",
    "            parts.append(f\"Subject: {ctx['subject']}\")\n",
    "        if ctx.get(\"image_description\"):\n",
    "            parts.append(f\"Image Description: {ctx['image_description']}\")\n",
    "        context_text = \"\\n\".join(parts)\n",
    "\n",
    "    elif task == \"IELTS_TASK2_POLISH\":\n",
    "        # Task2 只有 topic\n",
    "        topic = ctx.get(\"topic\", \"\")\n",
    "        context_text = f\"Topic: {topic}\" if topic else \"\"\n",
    "\n",
    "    else:\n",
    "        # fallback（理论不会走到这里）\n",
    "        context_text = \"\"\n",
    "\n",
    "    # 构造 prompt\n",
    "    prompt = f\"\"\"[TASK: {task}]\n",
    "\n",
    "Rewrite the following student essay to improve clarity, coherence, grammar, and lexical sophistication.\n",
    "Then provide specific suggestions explaining the main improvements.\n",
    "\n",
    "[CONTEXT]\n",
    "{context_text}\n",
    "\n",
    "[ORIGINAL]\n",
    "{example['original']}\n",
    "\n",
    "[OUTPUT]\n",
    "\"\"\"\n",
    "\n",
    "    # 建议部分\n",
    "    suggestions = \"\\n\".join(f\"- {s}\" for s in example[\"suggestions\"])\n",
    "\n",
    "    completion = f\"\"\"[POLISHED]\n",
    "{example['polished']}\n",
    "\n",
    "[SUGGESTIONS]\n",
    "{suggestions}\n",
    "\"\"\"\n",
    "\n",
    "    return {\"prompt\": prompt, \"completion\": completion}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe0ef3d-da1b-48fb-8507-439a28c7ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "task1 = load_dataset(\"json\", data_files=\"task1_converted.jsonl\")[\"train\"]\n",
    "task2 = load_dataset(\"json\", data_files=\"task2_converted.jsonl\")[\"train\"]\n",
    "\n",
    "dataset = concatenate_datasets([task1, task2]).shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02821e33-fd35-4782-aa2e-dcd9ce257b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c98a5fdf-1ae9-4047-9ac2-fa580f1dada5",
   "metadata": {},
   "source": [
    "##############测试用###########################\n",
    "train_dataset = dataset[\"train\"].select(range(500))   # 只保留前 500 条\n",
    "eval_dataset = dataset[\"test\"].select(range(100))     # eval 可以选 100 条，不要太少\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d84808-dbaa-4ea4-9061-e4a3aa043231",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(format_example)\n",
    "eval_dataset = eval_dataset.map(format_example)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36e0adfe-12d7-41de-90ad-1accedc6666d",
   "metadata": {},
   "source": [
    "train_dataset[19]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cf509e4-eed3-4511-843d-0ee4782509ab",
   "metadata": {},
   "source": [
    "print(train_dataset[19][\"context\"])\n",
    "print(\"----- PROMPT -----\")\n",
    "print(train_dataset[19][\"prompt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a571cf-aaa0-4fa4-8ec0-d646d81d812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [\"prompt\", \"completion\"]\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(\n",
    "    [col for col in train_dataset.column_names if col not in keep_cols]\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.remove_columns(\n",
    "    [col for col in eval_dataset.column_names if col not in keep_cols]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c3528fe-462f-448e-8363-94b95191644d",
   "metadata": {},
   "source": [
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a6627f-23ad-4c7e-bce1-c2f68c85a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import csv\n",
    "import os\n",
    "\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, save_path=\"loss_log.csv\"):\n",
    "        self.save_path = save_path\n",
    "        # 初始化文件并写入表头\n",
    "        with open(self.save_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"step\", \"train_loss\", \"eval_loss\"])\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None:\n",
    "            return\n",
    "\n",
    "        # 记录训练 loss\n",
    "        train_loss = logs.get(\"loss\")\n",
    "        eval_loss = logs.get(\"eval_loss\")\n",
    "        step = state.global_step\n",
    "\n",
    "        with open(self.save_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([step, train_loss, eval_loss])\n",
    "loss_logger = LossLoggerCallback(save_path=\"/root/autodl-tmp/loss_log_epoch_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0674d94-a988-4951-98d7-a9f6b51e7f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1488/907138681.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 1: tokenize function\n",
    "# ------------------------------------------------------\n",
    "def tokenize_example(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    completion = example[\"completion\"]\n",
    "\n",
    "    # 1. tokenize prompt\n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=True)\n",
    "    # 2. tokenize completion\n",
    "    completion_ids = tokenizer(completion, add_special_tokens=False)\n",
    "\n",
    "    input_ids = prompt_ids[\"input_ids\"] + completion_ids[\"input_ids\"]\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # 3. 构建 labels：prompt 部分全部 mask 掉（-100）\n",
    "    labels = [-100] * len(prompt_ids[\"input_ids\"]) + completion_ids[\"input_ids\"]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_example, remove_columns=train_dataset.column_names)\n",
    "eval_tokenized = eval_dataset.map(tokenize_example, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 2: Data Collator（自动 pad）\n",
    "# ------------------------------------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 3: TrainingArguments\n",
    "# ------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/root/autodl-tmp/sft-output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,   # 等效 batch=16\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",   # 避免自动连 wandb\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 4: Trainer 实例\n",
    "# ------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[loss_logger]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ccec11-cf1b-4979-9ba6-c540e59bae9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='601' max='1264' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 601/1264 1:22:07 < 1:30:53, 0.12 it/s, Epoch 0.95/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.524600</td>\n",
       "      <td>1.435470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.159400</td>\n",
       "      <td>1.136895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.107300</td>\n",
       "      <td>1.072289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.031800</td>\n",
       "      <td>1.036560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.012300</td>\n",
       "      <td>1.010856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.992202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.986300</td>\n",
       "      <td>0.977367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.988000</td>\n",
       "      <td>0.965536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.918700</td>\n",
       "      <td>0.956269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.934300</td>\n",
       "      <td>0.948195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.952500</td>\n",
       "      <td>0.941965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='532' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/532 00:22 < 00:47, 7.55 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# Step 5: 开始训练\n",
    "# ------------------------------------------------------\n",
    "trainer.train()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 6: 保存 LoRA Adapter\n",
    "# ------------------------------------------------------\n",
    "trainer.save_model(\"/root/autodl-tmp/sft-output/lora_epoch_2\")\n",
    "print(\"Training Finished. LoRA saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b0fb9-0af6-4a7a-a922-a3c48d8acc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"/usr/bin/shutdown -h now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce7c27d-29b7-4e78-929d-de523b862ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inference_prompt(sample):\n",
    "    task = sample[\"task\"]\n",
    "    ctx = sample[\"context\"]\n",
    "\n",
    "    # ===== 构造上下文 =====\n",
    "    if task == \"IELTS_TASK1_POLISH\":\n",
    "        parts = []\n",
    "        if ctx.get(\"topic\"):\n",
    "            parts.append(f\"Topic: {ctx['topic']}\")\n",
    "        if ctx.get(\"subject\"):\n",
    "            parts.append(f\"Subject: {ctx['subject']}\")\n",
    "        if ctx.get(\"image_description\"):\n",
    "            parts.append(f\"Image Description: {ctx['image_description']}\")\n",
    "        context_text = \"\\n\".join(parts)\n",
    "\n",
    "    elif task == \"IELTS_TASK2_POLISH\":\n",
    "        parts = []\n",
    "        if ctx.get(\"topic\"):\n",
    "            parts.append(f\"Topic: {ctx['topic']}\")\n",
    "        context_text = \"\\n\".join(parts)\n",
    "\n",
    "    else:\n",
    "        context_text = \"\"\n",
    "\n",
    "    # ===== 构造 prompt =====\n",
    "    prompt = f\"\"\"[TASK: {task}]\n",
    "\n",
    "Rewrite the following student essay to improve clarity, coherence, grammar, and lexical sophistication.\n",
    "Then provide specific suggestions explaining the main improvements.\n",
    "\n",
    "[CONTEXT]\n",
    "{context_text}\n",
    "\n",
    "[ORIGINAL]\n",
    "{sample['original']}\n",
    "\n",
    "[OUTPUT]\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bd29f-1acf-42ee-8880-45d4e070afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "@torch.no_grad()\n",
    "def generate(messages):\n",
    "    model_input = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        model_input,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    # Only decode newly generated tokens\n",
    "    new_tokens = output.sequences[:, model_input.shape[1]:]\n",
    "    text = tokenizer.decode(new_tokens[0], skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e349d5-0f6d-4971-ba5d-f67df50496df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample = dataset[\"test\"][1]   # 或 eval_dataset[i]\n",
    "prompt = build_inference_prompt(sample)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "print(prompt)\n",
    "output = generate(messages)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a5b01-6ac0-476b-94c7-99718fad804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"what is love?\"}\n",
    "]\n",
    "print(messages)\n",
    "output = generate(messages)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66341eb6-d241-4491-9fa5-56007220048f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
